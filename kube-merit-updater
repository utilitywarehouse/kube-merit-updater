#!/usr/bin/env bash

# Shell style guide: https://google.github.io/styleguide/shell.xml

# ## (-)  master and worker steps
# ## (*)  worker specific steps
#
# Assumes a kubernetes cluster with master and workers nodes.
#
# Performs the following actions concurrently, up to $max_nodes at a time:
# - drain nodes
# - reboot nodes and wait until they are back and in `Ready` state

set -o errexit
set -o nounset
set -o pipefail

readonly retire_time=$(date +"%Y-%m-%dT%H-%M-%SZ")

# flags
kube_context=''
role=''
resume=''
timeout=''
socks5_proxy=''
max_nodes=1

# Static vars
HTTP_PROXY=''

usage() {
  cat <<EOF
Usage: $0 -c <kube_context> -s [retire_time_label] -r [role]
        -t [timeout] -p [socks5_proxy] -n [max_nodes]

  -s    Resume node rolling. Argument is retire_time label. If set, -r [role]
        must also be set
  -t    Node drain timeout. How long to wait for the node to drain before
        shutting it down (in seconds, default 600s*max_nodes)
  -p    Specify a socks5 proxy for kubectl commands <address>:<port>
  -n    The maximum number of nodes that are permitted to be rolled at once (default 1)
EOF
}

while getopts 'hc:r:s:t:p:n:' flag; do
  case "${flag}" in
    c) kube_context="${OPTARG}" ;;
    r) role="${OPTARG}" ;;
    s) resume="${OPTARG}" ;;
    t) timeout="${OPTARG}" ;;
    p) socks5_proxy="${OPTARG}" ;;
    n) max_nodes="${OPTARG}" ;;
    h) usage && exit 0 ;;
    *) echo "Unexpected option: ${flag}" && usage && exit 1 ;;
  esac
done

### Deps
if [[ "${BASH_VERSINFO:-0}" -lt 4 ]]; then
    echo "Bash version must be >=4"
    exit 1
fi

deps="
  awk
  jq
  kubectl
  ssh
  timeout
  uniq
  xargs
"

missing_deps=""
for d in ${deps}; do
  if ! command -v "${d}" &> /dev/null; then
    missing_deps+="${d} "
  fi
done

if [[ -n "${missing_deps}" ]]; then
  echo "Missing dependencies: ${missing_deps}"
  exit 1
fi

## Set HTTP Proxy env var to proxy kubectl commands
if [[ -z "${socks5_proxy}" ]]; then
  echo "No socks proxy set"
else
  HTTP_PROXY="socks5://${socks5_proxy}"
fi
export HTTP_PROXY=${HTTP_PROXY}

# If a timeout isn't set then default to 10m * the max number of nodes that may
# be draining concurrently. This affords the same timeout to pods covered by the
# same PDB.
if [[ -z "${timeout}" ]]; then
  timeout=$((600*max_nodes))
fi

### Validation
if [[ -z "${kube_context}" ]]; then
  usage
  exit 1
fi

if [[ "${max_nodes}" -lt 1 ]]; then
  echo "Max nodes must be >=1: ${max_nodes}"
  usage
  exit 1
fi

if [[ -z "${role}" ]] ; then
  echo "A role should always be provided"
  usage
  exit 1
fi

function fail() {
  echo "$1" >&2
  exit 1
}

function retry() {
  local n=1
  local max=12
  local delay=8
  while true; do
    if "$@"; then
      break
    else
      if [[ $n -lt $max ]]; then
        ((n++))
        echo "command failed: attempt=$n max=$max"
        sleep $delay;
      else
        fail "the command has failed after $n attempts."
      fi
    fi
  done
}

label_for_cycling() {
  local role=$1
  local nodes=""
  while [[ -z "${nodes}" ]]; do
    nodes=$(retry kubectl --context="${kube_context}" get nodes -l role="${role}" -o json | jq -r '.items[].metadata.name')
  done

  echo "${kube_context}: nodes=$(echo "${nodes}" | wc -l) role=${role}"
  echo "labelling for retirement: role=${role}"
  for node in ${nodes}; do
    retry kubectl --context="${kube_context}" label node "${node}" retiring="${retire_time}" --overwrite=true
  done
}

delete_retirement_label() {
  local node=$1
  echo "removing labelling for retirement: role=${role}"
  retry kubectl --context="${kube_context}" label node "${node}" retiring-
}

delete_pods() {
  local node=$1

  retry timeout 60 kubectl --context="${kube_context}" get pod --all-namespaces \
    -o jsonpath="{range .items[?(.spec.nodeName==\"${node}\")]}{@.metadata.namespace} {.metadata.name} {end}" \
      | xargs -n2 \
      | xargs -I % sh -c "kubectl --context=${kube_context} delete pods -n=%"
}

drain_node() {
  local node=$1

  set +e
  time timeout --foreground ${timeout} kubectl --context="${kube_context}" drain "${node}" --ignore-daemonsets --force --delete-emptydir-data
  local rc=$?
  if [[ ${rc} -eq 0 ]]; then
    echo "drained successfully"
  elif [[ ${rc} -eq 124 ]]; then
    echo "timeout reached, continuing: timeout=${timeout}"
    delete_pods "${node}"
  else
    echo "kubectl drain exit error: ${rc}"
    delete_pods "${node}"
  fi
}

wait_until_volumeattachments_drained() {
  local node=$1

  until [[ $(kubectl --context="${kube_context}" get volumeattachment | grep -c "${node}") == 0 ]]; do
    echo "Waiting for kubelet to detach all volumes from node: ${node}..."
    sleep 1
  done
}

reboot_node() {
  local node=$1

  set +e

  ssh -o ServerAliveInterval=1 -o ServerAliveCountMax=1 -o BatchMode=yes "${node}" sudo reboot

  local kube_active
  kube_active=$(ssh -oBatchMode=yes "${node}" systemctl is-active kubelet.service)
  until [[ ${kube_active} == "active" ]]; do
    echo "awaiting node: ${node} to reboot and kubelet.service to become active"
    sleep 15
    kube_active=$(ssh -oBatchMode=yes "${node}" systemctl is-active kubelet.service)
  done
}

wait_for_ready_node() {
  local node=$1

  set +e
  # Wait for node to report ready again
  local node_status
  node_status=$(kubectl --context="${kube_context}" get node | grep "${node}" | awk '{print $2}')
  until [[ ${node_status} == "Ready,SchedulingDisabled" || ${node_status} == "Ready" ]]; do
    echo "awaiting node: ${node} to become Ready"
    sleep 10
    node_status=$(kubectl --context="${kube_context}" get node | grep "${node}" | awk '{print $2}')
  done
  # Uncordon the node
  retry kubectl --context="${kube_context}" uncordon "${node}"
}

cycle_node() {
    local node=$1

    drain_node "${node}"
    wait_until_volumeattachments_drained "${node}"
    reboot_node "${node}"
    wait_for_ready_node "${node}"
    delete_retirement_label "${node}"
}

cycle_nodes() {
  local role=$1
  local retiring=$2

  nodes=$(retry kubectl --context="${kube_context}" get nodes -l role="${role}",retiring="${retiring}" -o json |\
    jq -r '.items[].metadata.name')
  for node in ${nodes}; do
    while [[ $(jobs -p | wc -l) -ge $max_nodes ]]; do
      wait -n
    done
    # Run cycle_node in a background job. Redirect stdout and stderr through
    # 'logger' which adds a timestamp and the node name to each line of output.
    cycle_node "${node}" > >(logger "${node}") 2> >(logger "${node}" >&2) &
  done
  wait
}

log() {
  echo -e "${*}" | logger >&2
}

logger() {
    while read -r line; do
      printf "%(%Y-%m-%dT%H:%M:%S)T [${1:-main}] %s\n" '-1' "${line}"
    done
}

update() {
  local role=$1

  label_for_cycling "${role}"
  cycle_nodes "${role}" "${retire_time}"
}

resume() {
  local role=$1

  cycle_nodes "${role}" "${resume}"
}

# Cleanup child processes on exit
trap "kill 0" EXIT

log "kube cluster: ${kube_context}"

if [[ -n "${resume}" ]]; then
  resume "${role}"
  exit 0
fi

update "${role}"

log "run: result=\"success\""

